{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d90a11d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm as tqdm\n",
    "from scipy.special import softmax\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "from captum.attr import visualization\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DefaultDataCollator\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric \n",
    "from datasets import list_datasets, list_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "be4c293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model   \n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"csarron/bert-base-uncased-squad-v1\").to(\"cuda\")\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\").to(\"cuda\")\n",
    "# model = AutoModelForQuestionAnswering.from_pretrained(\"thatdramebaazguy/roberta-base-squad\").to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"csarron/bert-base-uncased-squad-v1\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained('thatdramebaazguy/roberta-base-squad')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased-distilled-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "386a3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1fa18623",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = {101,102}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "294f621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8964566",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2de655b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_num = 200\n",
    "to_test = np.array(tokenized_squad['validation'])\n",
    "# to_test_idx = np.random.choice(len(tokenized_squad['validation']), test_num, replace=False)\n",
    "# to_test = to_test[to_test_idx]\n",
    "# len(to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e251f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sample(tokenized_squad,index):\n",
    "    input_ids = tokenized_squad['validation'][index]['input_ids']\n",
    "    text_ids = (torch.tensor([input_ids])).to(\"cuda\")\n",
    "    text_words = tokenizer.convert_ids_to_tokens(text_ids[0])\n",
    "    \n",
    "    att_mask = tokenized_squad['validation'][index]['attention_mask']\n",
    "    special_idxs = [x for x, y in list(enumerate(input_ids)) if y in special_tokens]\n",
    "    att_mask = [0 if index in special_idxs else 1 for index, item in enumerate(att_mask)]\n",
    "    att_mask = (torch.tensor([att_mask])).to(\"cuda\")\n",
    "    \n",
    "    start_positions = tokenized_squad['validation'][index]['start_positions']\n",
    "    end_positions = tokenized_squad['validation'][index]['end_positions']\n",
    "    \n",
    "    return text_ids, att_mask, text_words, start_positions, end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4cf444f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cat(model, text_ids, att_mask, is_relu = False, is_start = True):\n",
    "    \n",
    "    # outputs\n",
    "    result = model(text_ids, att_mask, output_hidden_states=True, output_attentions=True)\n",
    "    \n",
    "    # attention blocks\n",
    "    blocks = model.bert.encoder.layer\n",
    "    # blocks = model.distilbert.transformer.layer # cy\n",
    "    # blocks = model.roberta.encoder.layer # cy\n",
    "    \n",
    "\n",
    "    for blk_id in range(len(blocks)):\n",
    "        result.hidden_states[blk_id].retain_grad()\n",
    "        \n",
    "    start_prob = result['start_logits'][0]\n",
    "    start_idx = torch.argmax(start_prob).cpu().detach().numpy()\n",
    "    end_prob = result['end_logits'][0]\n",
    "    end_idx = torch.argmax(end_prob).cpu().detach().numpy()\n",
    "\n",
    "    model.zero_grad()\n",
    "    if is_start:\n",
    "        start_prob[start_idx].backward() \n",
    "    else:\n",
    "        end_prob[end_idx].backward() \n",
    "        \n",
    "    cat_layers = {}\n",
    "    \n",
    "    for blk_id in range(len(blocks)):\n",
    "        hs_grad = result.hidden_states[blk_id].grad\n",
    "        \n",
    "        att = result.attentions[blk_id].squeeze(0)\n",
    "        att = att.mean(dim=0)\n",
    "        att = att.mean(dim=0)\n",
    "        \n",
    "        cat_layer = (hs_grad * result.hidden_states[blk_id]).sum(dim=-1).squeeze(0)\n",
    "        cat_layer = cat_layer * att\n",
    "\n",
    "        cat_layers[blk_id] = cat_layer\n",
    "        \n",
    "    cat = sum(cat_layers.values())  \n",
    "    \n",
    "    if is_relu:\n",
    "        cat_expln = torch.relu(cat)\n",
    "    else:\n",
    "        cat_expln = cat\n",
    "        \n",
    "    # cat_expln  = torch.abs(cat)     \n",
    "        \n",
    "    cat_expln = cat_expln.cpu().detach().numpy()\n",
    "        \n",
    "    return cat_expln, start_idx, end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6adeee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_att_layers(model, text_ids, att_mask):\n",
    "    \n",
    "    # outputs\n",
    "    result = model(text_ids, att_mask, output_hidden_states=True, output_attentions=True)\n",
    "    \n",
    "    # attention blocks\n",
    "    blocks = model.bert.encoder.layer\n",
    "    # blocks = model.distilbert.transformer.layer # cy\n",
    "    # blocks = model.roberta.encoder.layer # cy\n",
    "    \n",
    "    att_layers = {}\n",
    "    \n",
    "    for blk_id in range(len(blocks)):\n",
    "        att = result.attentions[blk_id].squeeze(0)\n",
    "        att = att.cpu().detach().numpy()\n",
    "        att_layers[blk_id] = att\n",
    "        \n",
    "    return att_layers, blocks\n",
    "\n",
    "\n",
    "def generate_att_mat(att_layers, blocks, avg_head=True):\n",
    "    \n",
    "    att_mat = {}\n",
    "    \n",
    "    if avg_head:\n",
    "        for blk_id in range(len(blocks)):\n",
    "            att = att_layers[blk_id]\n",
    "            att = np.mean(att, axis = 0) # average over head \n",
    "            att_mat[blk_id] = att\n",
    "            \n",
    "    else:\n",
    "        for blk_id in range(len(blocks)):\n",
    "            head_explns = {}  \n",
    "            att = att_layers[blk_id]\n",
    "            for i in range (att.shape[0]):\n",
    "                head_explns[i] = att[i]\n",
    "            att_mat[blk_id] = head_explns\n",
    "            \n",
    "    att_mat = np.array(list(att_mat.values()))\n",
    "    \n",
    "    # att_mat.shape: (6, 9, 9)\n",
    "    \n",
    "    return att_mat\n",
    "\n",
    "\n",
    "def get_raw_att(att_mat, layer=-1):\n",
    "    raw_att = np.mean(att_mat, axis = 0) # average over tokens\n",
    "    return raw_att[layer]\n",
    "\n",
    "\n",
    "def compute_rollout_attention(att_mat):\n",
    "    \n",
    "    residual_att = np.eye(att_mat.shape[1])[None,...]\n",
    "    \n",
    "    aug_att_mat = att_mat + residual_att\n",
    "    aug_att_mat = aug_att_mat / aug_att_mat.sum(axis=-1)[...,None]\n",
    "        \n",
    "    joint_attentions = np.zeros(att_mat.shape)\n",
    "    n_layers = joint_attentions.shape[0]\n",
    "    joint_attentions[0] = att_mat[0]\n",
    "    \n",
    "    for i in np.arange(1,n_layers):\n",
    "        joint_attentions[i] = att_mat[i].dot(joint_attentions[i-1])\n",
    "    \n",
    "    rollout_expln = np.mean(joint_attentions, axis = 0) # average over layers \n",
    "    rollout_expln = np.mean(rollout_expln, axis = 0) # average over tokens \n",
    "        \n",
    "    return rollout_expln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fe6e5cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_count(start_positions,end_positions, attribution):\n",
    "    answers = []\n",
    "    sorted_idx = np.argsort(-attribution)\n",
    "    sorted_idx = set(sorted_idx[:20])\n",
    "    \n",
    "    for i in range(start_positions,end_positions+1):\n",
    "        answers.append(i)\n",
    "        \n",
    "    count = 0\n",
    "    \n",
    "    for a in answers:\n",
    "        if a in sorted_idx:\n",
    "            count +=1 \n",
    "        \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "53fa38e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perfect_count(start_positions,end_positions):\n",
    "    \n",
    "    answers = []\n",
    "    for i in range(start_positions,end_positions+1):\n",
    "        answers.append(i)\n",
    "        \n",
    "    count = len(answers)\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b861d291",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_instance(instance):\n",
    "    input_ids = instance['input_ids']\n",
    "    text_ids = (torch.tensor([input_ids])).to(\"cuda\")\n",
    "    text_words = tokenizer.convert_ids_to_tokens(text_ids[0])\n",
    "    \n",
    "    att_mask = instance['attention_mask']\n",
    "    special_idxs = [x for x, y in list(enumerate(input_ids)) if y in special_tokens]\n",
    "    att_mask = [0 if index in special_idxs else 1 for index, item in enumerate(att_mask)]\n",
    "    att_mask = (torch.tensor([att_mask])).to(\"cuda\")\n",
    "    \n",
    "    start_positions = instance['start_positions']\n",
    "    end_positions = instance['end_positions']\n",
    "    \n",
    "    return text_ids, text_words, att_mask, start_positions, end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "567bab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_test(model, test_data):\n",
    "    \n",
    "    cat_scores = []\n",
    "    att_scores = []\n",
    "    rollout_scores = []\n",
    "    \n",
    "    perfect_scores = []\n",
    "    \n",
    "    for i, test_instance in enumerate(test_data):\n",
    "        text_ids, text_words, att_mask, start_positions, end_positions = preprocess_instance(test_instance)\n",
    "        \n",
    "        cat_expln_start, start_idx, end_idx = generate_cat(model, text_ids, att_mask, is_relu = False, is_start = True)\n",
    "        cat_expln_end, start_idx, end_idx = generate_cat(model, text_ids, att_mask, is_relu = False, is_start = False)\n",
    "        \n",
    "        cat_expln = cat_expln_start + cat_expln_end\n",
    "        \n",
    "        att_layers, blocks = generate_att_layers(model, text_ids, att_mask)\n",
    "        att_mat = generate_att_mat(att_layers, blocks, avg_head= True)\n",
    "        raw_att_expln = get_raw_att(att_mat, layer=-1)\n",
    "        rollout_expln = compute_rollout_attention(att_mat)\n",
    "        \n",
    "        count_cat = cal_count(start_positions,end_positions,cat_expln)\n",
    "        count_att = cal_count(start_positions,end_positions,raw_att_expln)\n",
    "        count_rollout = cal_count(start_positions,end_positions,rollout_expln)\n",
    "        \n",
    "        cat_scores.append(count_cat)\n",
    "        att_scores.append(count_att)\n",
    "        rollout_scores.append(count_rollout)\n",
    "        \n",
    "        p_count = perfect_count(start_positions,end_positions)\n",
    "        perfect_scores.append(p_count)\n",
    "    \n",
    "    cat_score = np.mean(cat_scores)\n",
    "    att_score = np.mean(att_scores)\n",
    "    rollout_score = np.mean(rollout_scores)\n",
    "        \n",
    "    perfect_score = np.mean(perfect_scores)\n",
    "    \n",
    "    return cat_score, att_score, rollout_score, perfect_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c77b6113",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_score, att_score, rollout_score, perfect_score = qa_test(model, to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb69a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cat_score, att_score, rollout_score, perfect_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38268c5b",
   "metadata": {},
   "source": [
    "# Visulize Single Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "31da38cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ids, att_mask, text_words, start_positions, end_positions = preprocess_sample(tokenized_squad,index=0)\n",
    "outputs = model(text_ids,att_mask,output_hidden_states=True,output_attentions=True)\n",
    "result = model(text_ids, att_mask, output_hidden_states=True, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d8b2e396",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_expln_start, start_idx, end_idx = generate_cat(model, text_ids, att_mask, is_relu = False, is_start = True)\n",
    "cat_expln_end, start_idx, end_idx = generate_cat(model, text_ids, att_mask, is_relu = False, is_start = True)\n",
    "cat_expln = cat_expln_start + cat_expln_end\n",
    "\n",
    "att_layers, blocks = generate_att_layers(model, text_ids, att_mask)\n",
    "att_mat = generate_att_mat(att_layers, blocks, avg_head= True)\n",
    "\n",
    "raw_att_expln = get_raw_att(att_mat, layer=-1)\n",
    "rollout_expln = compute_rollout_attention(att_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8ec5c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_text_attr(expln,str_list,is_relu = True):\n",
    "    if is_relu:\n",
    "        rgb = lambda x: '0,0,0' if x < 0 else '0,255,0'\n",
    "        alpha = lambda x: max(x, 0) * 30\n",
    "    else:\n",
    "        rgb = lambda x: '255,0,0' if x < 0 else '0,255,0'\n",
    "        alpha = lambda x: x * -50 if x < 0 else x * 50\n",
    "    attrs = list(expln)\n",
    "    subwords = str_list\n",
    "    \n",
    "    token_marks = [\n",
    "        f'<mark style=\"background-color:rgba({rgb(attr)},{alpha(attr)})\">{token}</mark>'\n",
    "        for token, attr in zip(subwords, attrs)\n",
    "    ]\n",
    "    \n",
    "    display(HTML('<p>' + ' '.join(token_marks) + '</p>'))\n",
    "    \n",
    "    return token_marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01d9401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_marks = show_text_attr(cat_expln,text_words,is_relu = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f528138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_text_attr(raw_att_expln,text_words,is_relu = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb55b2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_text_attr(rollout_expln,text_words,is_relu = True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6638a2cf1d43c3a13d1389d136f8cbd4a3a87bb07312d70c2fb91fa82cf84aeb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
